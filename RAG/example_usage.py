"""
RAG ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì œ

ê°ì • ê¸°ë°˜ ì²­í‚¹ê³¼ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from RAG.emotion_chunker import split_text_by_emotions
from RAG.vector_store import EmotionAwareVectorStore
from RAG.retriever import EmotionAwareRetriever
from RAG.utils import (
    load_text_from_file,
    save_chunks_to_json,
    get_emotion_statistics,
    visualize_emotional_arc
)


def example_basic_chunking():
    """ê¸°ë³¸ ì²­í‚¹ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ì˜ˆì œ 1: ê¸°ë³¸ ê°ì • ê¸°ë°˜ ì²­í‚¹")
    print("=" * 60)

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    sample_text = """
    ì˜¤ëŠ˜ì€ ì •ë§ í–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤. ì¹œêµ¬ë“¤ê³¼ ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ëƒˆê³ ,
    ë§›ìˆëŠ” ìŒì‹ë„ ë¨¹ì—ˆë‹¤. ëª¨ë“  ê²ƒì´ ì™„ë²½í–ˆë‹¤.

    ê·¸ëŸ¬ë‚˜ ì§‘ì— ëŒì•„ì˜¤ëŠ” ê¸¸ì— ë‚˜ìœ ì†Œì‹ì„ ë“¤ì—ˆë‹¤.
    í• ë¨¸ë‹ˆê»˜ì„œ í¸ì°®ìœ¼ì‹œë‹¤ëŠ” ì „í™”ë¥¼ ë°›ì•˜ë‹¤.
    ìˆœê°„ ì„¸ìƒì´ ë¬´ë„ˆì§€ëŠ” ê²ƒ ê°™ì•˜ë‹¤.

    ë³‘ì›ìœ¼ë¡œ ë‹¬ë ¤ê°”ë‹¤. ë‹¤í–‰íˆ í° ë¬¸ì œëŠ” ì•„ë‹ˆì—ˆë‹¤.
    ì•ˆë„ì˜ í•œìˆ¨ì„ ì‰¬ì—ˆë‹¤. ê°€ì¡±ì´ ì–¼ë§ˆë‚˜ ì†Œì¤‘í•œì§€ ë‹¤ì‹œ ê¹¨ë‹¬ì•˜ë‹¤.
    """

    # ì²­í‚¹ ì‹¤í–‰
    chunks = split_text_by_emotions(
        sample_text,
        max_chunk_size=200,
        min_chunk_size=50
    )

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±ë¨\n")

    for i, chunk in enumerate(chunks):
        print(f"ì²­í¬ {i + 1}:")
        print(f"  ê°ì •: {chunk.emotion}")
        print(f"  ìœ„ì¹˜: {chunk.start_pos}-{chunk.end_pos}")
        print(f"  í…ìŠ¤íŠ¸: {chunk.text[:50]}...")

        if chunk.metadata and chunk.metadata.get('is_transition_point'):
            print(f"  ğŸ”„ ì „í™˜ì ! ì¤‘ìš”ë„: {chunk.metadata.get('transition_significance')}")
            print(f"  ë‹¤ìŒ ê°ì •: {chunk.metadata.get('next_emotion')}")

        print()

    # í†µê³„ ì¶œë ¥
    stats = get_emotion_statistics(chunks)
    print("\nì²­í‚¹ í†µê³„:")
    print(f"  ì´ ì²­í¬ ìˆ˜: {stats['total_chunks']}")
    print(f"  í‰ê·  ê¸¸ì´: {stats['avg_chunk_length']:.1f}ì")
    print(f"  ê°ì • ë¶„í¬: {stats['emotion_distribution']}")

    return chunks


def example_vector_store():
    """ë²¡í„° ìŠ¤í† ì–´ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ì˜ˆì œ 2: ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ ë° ê²€ìƒ‰")
    print("=" * 60)

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    sample_text = """
    AI ê¸°ìˆ ì˜ ë°œì „ì€ ë†€ë¼ìš¸ ë”°ë¦„ì´ë‹¤. ë§¤ì¼ ìƒˆë¡œìš´ í˜ì‹ ì´ ì¼ì–´ë‚œë‹¤.
    íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ì„±ì¥ì€ ê²½ì´ë¡­ë‹¤.

    í•˜ì§€ë§Œ AI ìœ¤ë¦¬ ë¬¸ì œëŠ” ì—¬ì „íˆ ìš°ë ¤ìŠ¤ëŸ½ë‹¤.
    ê°œì¸ì •ë³´ ë³´í˜¸ì™€ í¸í–¥ì„± ë¬¸ì œê°€ ì‹¬ê°í•˜ë‹¤.
    ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ì§€ ì•Šìœ¼ë©´ í° ìœ„í—˜ì´ ë  ìˆ˜ ìˆë‹¤.

    ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  AIì˜ ê¸ì •ì  ê°€ëŠ¥ì„±ì€ ë¬´í•œí•˜ë‹¤.
    ì˜ë£Œ, êµìœ¡, í™˜ê²½ ë¬¸ì œ í•´ê²°ì— í° ë„ì›€ì´ ë  ê²ƒì´ë‹¤.
    ìš°ë¦¬ëŠ” í¬ë§ì„ ê°€ì§€ê³  ì•ìœ¼ë¡œ ë‚˜ì•„ê°€ì•¼ í•œë‹¤.
    """

    # 1. ì²­í‚¹
    print("\n1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹...")
    chunks = split_text_by_emotions(sample_text, max_chunk_size=300)

    # 2. ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    print("\n2ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”...")
    vector_store = EmotionAwareVectorStore(
        collection_name="example_collection",
        persist_directory="./example_chroma_db"
    )

    # 3. ì²­í¬ ì €ì¥
    print("\n3ë‹¨ê³„: ì²­í¬ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥...")
    vector_store.add_chunks(chunks, document_id="sample_doc")

    # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n4ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    queries = [
        "AIì˜ ê¸ì •ì ì¸ ì¸¡ë©´",
        "ìš°ë ¤ë˜ëŠ” ë¬¸ì œì ",
        "ë¯¸ë˜ì— ëŒ€í•œ í¬ë§"
    ]

    for query in queries:
        print(f"\nì¿¼ë¦¬: '{query}'")
        results = vector_store.search(query, k=2)

        for i, result in enumerate(results, 1):
            print(f"  ê²°ê³¼ {i}:")
            print(f"    í…ìŠ¤íŠ¸: {result['document'][:60]}...")
            print(f"    ê°ì •: {result['metadata'].get('emotion', 'N/A')}")
            print(f"    ê±°ë¦¬: {result['distance']:.4f}")

    # í†µê³„
    stats = vector_store.get_stats()
    print(f"\në²¡í„° ìŠ¤í† ì–´ í†µê³„: {stats}")

    return vector_store


def example_advanced_retrieval():
    """ê³ ê¸‰ ê²€ìƒ‰ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ì˜ˆì œ 3: ê³ ê¸‰ ê²€ìƒ‰ ì „ëµ")
    print("=" * 60)

    # ê¸´ ìƒ˜í”Œ í…ìŠ¤íŠ¸
    sample_text = """
    ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ê°€ ì‹œì‘ë˜ì—ˆë‹¤. íŒ€ì›ë“¤ì€ ëª¨ë‘ ì—´ì •ì ì´ì—ˆë‹¤.
    ìš°ë¦¬ëŠ” í˜ì‹ ì ì¸ ì œí’ˆì„ ë§Œë“¤ ê²ƒì´ë¼ê³  í™•ì‹ í–ˆë‹¤.

    ê°œë°œ ì´ˆê¸°ì—ëŠ” ëª¨ë“  ê²ƒì´ ìˆœì¡°ë¡œì› ë‹¤. í”„ë¡œí† íƒ€ì…ì€ ì™„ë²½í–ˆë‹¤.
    íˆ¬ììë“¤ì˜ ë°˜ì‘ë„ ì¢‹ì•˜ë‹¤. ë¯¸ë˜ê°€ ë°ì•„ ë³´ì˜€ë‹¤.

    ê·¸ëŸ¬ë‚˜ 3ê°œì›” í›„ ì‹¬ê°í•œ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤.
    í•µì‹¬ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì•˜ë‹¤. íŒ€ì˜ ì‚¬ê¸°ê°€ ê¸‰ê²©íˆ ë–¨ì–´ì¡Œë‹¤.
    ëª¨ë‘ê°€ ë¶ˆì•ˆí•´í–ˆë‹¤. í”„ë¡œì íŠ¸ê°€ ì‹¤íŒ¨í•  ê²ƒ ê°™ì•˜ë‹¤.

    í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” í¬ê¸°í•˜ì§€ ì•Šì•˜ë‹¤. ë°¤ë‚®ìœ¼ë¡œ ë¬¸ì œë¥¼ ë¶„ì„í–ˆë‹¤.
    ë§ˆì¹¨ë‚´ í•´ê²°ì±…ì„ ì°¾ì•„ëƒˆë‹¤. ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì´ íš¨ê³¼ê°€ ìˆì—ˆë‹¤.

    6ê°œì›” í›„ ì œí’ˆì„ ì„±ê³µì ìœ¼ë¡œ ì¶œì‹œí–ˆë‹¤. ì‚¬ìš©ì ë°˜ì‘ì´ í­ë°œì ì´ì—ˆë‹¤.
    ìš°ë¦¬ì˜ ë…¸ë ¥ì´ ê²°ì‹¤ì„ ë§ºì—ˆë‹¤. ì´ë³´ë‹¤ ë” ë³´ëŒì°¬ ìˆœê°„ì€ ì—†ì—ˆë‹¤.
    """

    # ì²­í‚¹ ë° ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
    chunks = split_text_by_emotions(sample_text, max_chunk_size=200)

    vector_store = EmotionAwareVectorStore(
        collection_name="advanced_example",
        persist_directory="./example_chroma_db"
    )
    vector_store.add_chunks(chunks, document_id="project_story")

    # ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    retriever = EmotionAwareRetriever(vector_store)

    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ í…ŒìŠ¤íŠ¸
    query = "í”„ë¡œì íŠ¸ì˜ ìœ„ê¸° ìƒí™©"

    print(f"\nì¿¼ë¦¬: '{query}'\n")

    # ì „ëµ 1: ê¸°ë³¸ ê²€ìƒ‰
    print("ì „ëµ 1: ê¸°ë³¸ ê²€ìƒ‰")
    basic_results = retriever.retrieve(query, k=2, strategy="basic")
    for i, r in enumerate(basic_results, 1):
        print(f"  {i}. {r['document'][:50]}... (ê±°ë¦¬: {r['distance']:.4f})")

    # ì „ëµ 2: ê°ì • ë¶€ìŠ¤íŠ¸ ê²€ìƒ‰
    print("\nì „ëµ 2: ê°ì • ë¶€ìŠ¤íŠ¸ ê²€ìƒ‰")
    boosted_results = retriever.retrieve(query, k=2, strategy="emotion_boosted")
    for i, r in enumerate(boosted_results, 1):
        sig = r['metadata'].get('transition_significance', 0)
        print(f"  {i}. {r['document'][:50]}... (ì¤‘ìš”ë„: {sig})")

    # ì „ëµ 3: ë¬¸ë§¥ í¬í•¨ ê²€ìƒ‰
    print("\nì „ëµ 3: ë¬¸ë§¥ í¬í•¨ ê²€ìƒ‰")
    contextual_results = retriever.retrieve(query, k=1, strategy="contextual", context_window=1)
    if contextual_results:
        print(f"  ë¬¸ë§¥ í¬í•¨ í…ìŠ¤íŠ¸:\n  {contextual_results[0].get('context', '')[:100]}...")

    # ì „ëµ 4: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    print("\nì „ëµ 4: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    hybrid_results = retriever.retrieve(query, k=2, strategy="hybrid")
    for i, r in enumerate(hybrid_results, 1):
        print(f"  {i}. {r['document'][:50]}... (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {r.get('hybrid_score', 0):.4f})")

    # ê°ì • ì•„í¬ ì¶”ì¶œ
    print("\nê°ì • íë¦„ ë¶„ì„:")
    emotional_arc = retriever.get_emotional_arc("project_story")
    for chunk_info in emotional_arc[:5]:  # ì²˜ìŒ 5ê°œë§Œ
        print(f"  ì²­í¬ {chunk_info['chunk_id']}: {chunk_info['emotion']}")

    # ì‹œê°í™”
    print("\n" + visualize_emotional_arc(chunks))

    return retriever


def example_with_real_file():
    """ì‹¤ì œ íŒŒì¼ë¡œ ì‘ì—…í•˜ëŠ” ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ì˜ˆì œ 4: ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬")
    print("=" * 60)

    # íŒŒì¼ ê²½ë¡œ (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ)
    file_path = "./sample_text.txt"

    if not Path(file_path).exists():
        print(f"\nâš ï¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        print("ìƒ˜í”Œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")

        # ìƒ˜í”Œ íŒŒì¼ ìƒì„±
        sample_content = """
        ì—¬í–‰ì˜ ì‹œì‘ì€ ì„¤ë ˜ìœ¼ë¡œ ê°€ë“í–ˆë‹¤.
        ìƒˆë¡œìš´ ê³³ì„ íƒí—˜í•œë‹¤ëŠ” ìƒê°ë§Œìœ¼ë¡œë„ ê°€ìŠ´ì´ ë›°ì—ˆë‹¤.

        í•˜ì§€ë§Œ ì²«ë‚ ë¶€í„° ë¬¸ì œê°€ ìƒê²¼ë‹¤.
        ì§ì„ ìƒì–´ë²„ë ¸ê³ , ìˆ™ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆë‹¤.

        ë‹¤í–‰íˆ ì¹œì ˆí•œ í˜„ì§€ì¸ì˜ ë„ì›€ì„ ë°›ì•˜ë‹¤.
        ê·¸ë‚  ë°¤, ì¸ê°„ì˜ ì„ ì˜ì— ëŒ€í•´ ë‹¤ì‹œ ìƒê°í•˜ê²Œ ë˜ì—ˆë‹¤.
        """

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(sample_content)

    # íŒŒì¼ ë¡œë“œ
    text = load_text_from_file(file_path)
    print(f"\níŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(text)}ì")

    # ì²­í‚¹
    chunks = split_text_by_emotions(text, max_chunk_size=500)

    # JSONìœ¼ë¡œ ì €ì¥
    output_path = "./chunks_output.json"
    save_chunks_to_json(chunks, output_path)

    print(f"\nì²­í¬ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

    # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
    vector_store = EmotionAwareVectorStore(
        collection_name="file_example",
        persist_directory="./example_chroma_db"
    )
    vector_store.add_chunks(chunks, document_id="travel_story")

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query = "ì—¬í–‰ ì¤‘ ì–´ë ¤ì›€"
    results = vector_store.search(query, k=2)

    print(f"\nê²€ìƒ‰ ê²°ê³¼ ('{query}'):")
    for i, result in enumerate(results, 1):
        print(f"\nê²°ê³¼ {i}:")
        print(f"  {result['document']}")
        print(f"  ê°ì •: {result['metadata'].get('emotion')}")


def main():
    """ëª¨ë“  ì˜ˆì œ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("ê°ì • ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì˜ˆì œ")
    print("=" * 60)

    try:
        # ì˜ˆì œ 1: ê¸°ë³¸ ì²­í‚¹
        example_basic_chunking()

        # ì˜ˆì œ 2: ë²¡í„° ìŠ¤í† ì–´
        example_vector_store()

        # ì˜ˆì œ 3: ê³ ê¸‰ ê²€ìƒ‰
        example_advanced_retrieval()

        # ì˜ˆì œ 4: íŒŒì¼ ì²˜ë¦¬
        example_with_real_file()

        print("\n" + "=" * 60)
        print("ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
