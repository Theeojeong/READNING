"""
RAG ì‹œìŠ¤í…œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

from typing import List, Dict, Any
import re
from pathlib import Path


def load_text_from_file(file_path: str) -> str:
    """
    íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        file_path: íŒŒì¼ ê²½ë¡œ

    Returns:
        í…ìŠ¤íŠ¸ ë‚´ìš©
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        # UTF-8 ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
        try:
            with open(file_path, 'r', encoding='cp949') as f:
                return f.read()
        except:
            raise ValueError(f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")


def save_chunks_to_json(chunks: List[Any], output_path: str) -> None:
    """
    ì²­í¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        chunks: EmotionChunk ê°ì²´ ë¦¬ìŠ¤íŠ¸
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    import json

    chunks_data = [
        chunk.to_dict() if hasattr(chunk, 'to_dict') else chunk
        for chunk in chunks
    ]

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì²­í¬ ì €ì¥ ì™„ë£Œ: {output_path}")


def load_chunks_from_json(json_path: str) -> List[Dict[str, Any]]:
    """
    JSON íŒŒì¼ì—ì„œ ì²­í¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        json_path: JSON íŒŒì¼ ê²½ë¡œ

    Returns:
        ì²­í¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    import json

    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤.

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)

    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì œí•œ
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


def split_into_sentences(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        text: ì „ì²´ í…ìŠ¤íŠ¸

    Returns:
        ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    # í•œê¸€/ì˜ì–´ ë¬¸ì¥ êµ¬ë¶„
    sentence_endings = r'[.!?ã€‚!?]\s+'
    sentences = re.split(sentence_endings, text)

    return [s.strip() for s in sentences if s.strip()]


def calculate_chunk_overlap(
    chunks: List[Any],
    overlap_chars: int = 50
) -> List[Dict[str, Any]]:
    """
    ì²­í¬ ê°„ ì˜¤ë²„ë©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        overlap_chars: ì˜¤ë²„ë©í•  ë¬¸ì ìˆ˜

    Returns:
        ì˜¤ë²„ë© ì •ë³´ê°€ ì¶”ê°€ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    result = []

    for i, chunk in enumerate(chunks):
        chunk_dict = chunk.to_dict() if hasattr(chunk, 'to_dict') else chunk
        chunk_dict['overlap_info'] = {
            'has_prev': i > 0,
            'has_next': i < len(chunks) - 1
        }

        if i > 0:
            prev_chunk = chunks[i - 1]
            prev_text = prev_chunk.text if hasattr(prev_chunk, 'text') else prev_chunk.get('text', '')
            chunk_dict['overlap_info']['prev_overlap'] = prev_text[-overlap_chars:]

        if i < len(chunks) - 1:
            next_chunk = chunks[i + 1]
            next_text = next_chunk.text if hasattr(next_chunk, 'text') else next_chunk.get('text', '')
            chunk_dict['overlap_info']['next_overlap'] = next_text[:overlap_chars]

        result.append(chunk_dict)

    return result


def merge_search_results(
    results1: List[Dict[str, Any]],
    results2: List[Dict[str, Any]],
    weight1: float = 0.5,
    weight2: float = 0.5
) -> List[Dict[str, Any]]:
    """
    ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.

    Args:
        results1: ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼
        results2: ë‘ ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼
        weight1: ì²« ë²ˆì§¸ ê²°ê³¼ì˜ ê°€ì¤‘ì¹˜
        weight2: ë‘ ë²ˆì§¸ ê²°ê³¼ì˜ ê°€ì¤‘ì¹˜

    Returns:
        ë³‘í•©ëœ ê²€ìƒ‰ ê²°ê³¼
    """
    # ID ê¸°ë°˜ ê²°ê³¼ ë§¤í•‘
    merged_dict = {}

    for result in results1:
        result_id = result.get('id', '')
        if result_id:
            merged_dict[result_id] = result.copy()
            merged_dict[result_id]['merged_score'] = result.get('distance', 0) * weight1

    for result in results2:
        result_id = result.get('id', '')
        if result_id:
            if result_id in merged_dict:
                # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•©
                merged_dict[result_id]['merged_score'] += result.get('distance', 0) * weight2
            else:
                # ìƒˆ ê²°ê³¼ ì¶”ê°€
                merged_dict[result_id] = result.copy()
                merged_dict[result_id]['merged_score'] = result.get('distance', 0) * weight2

    # ë³‘í•©ëœ ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
    merged_results = list(merged_dict.values())
    merged_results.sort(key=lambda x: x.get('merged_score', 0))

    return merged_results


def get_emotion_statistics(chunks: List[Any]) -> Dict[str, Any]:
    """
    ì²­í¬ë“¤ì˜ ê°ì • í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        chunks: EmotionChunk ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    if not chunks:
        return {"total": 0}

    emotions = []
    significances = []
    total_length = 0

    for chunk in chunks:
        if hasattr(chunk, 'emotion'):
            emotions.append(chunk.emotion)
            total_length += len(chunk.text)

            if hasattr(chunk, 'metadata') and chunk.metadata:
                sig = chunk.metadata.get('transition_significance')
                if sig:
                    significances.append(sig)
        elif isinstance(chunk, dict):
            emotions.append(chunk.get('emotion', 'unknown'))
            total_length += len(chunk.get('text', ''))

            sig = chunk.get('metadata', {}).get('transition_significance')
            if sig:
                significances.append(sig)

    # ê°ì • ë¶„í¬
    emotion_counts = {}
    for emotion in emotions:
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

    stats = {
        "total_chunks": len(chunks),
        "total_characters": total_length,
        "avg_chunk_length": total_length / len(chunks) if chunks else 0,
        "emotion_distribution": emotion_counts,
    }

    if significances:
        stats["avg_significance"] = sum(significances) / len(significances)
        stats["max_significance"] = max(significances)
        stats["transition_points"] = len(significances)

    return stats


def visualize_emotional_arc(chunks: List[Any]) -> str:
    """
    ê°ì • ì•„í¬ë¥¼ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

    Args:
        chunks: EmotionChunk ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì‹œê°í™” í…ìŠ¤íŠ¸
    """
    if not chunks:
        return "ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤."

    lines = ["ê°ì • íë¦„ ì‹œê°í™”:", "=" * 50]

    for i, chunk in enumerate(chunks):
        emotion = chunk.emotion if hasattr(chunk, 'emotion') else chunk.get('emotion', 'unknown')
        text_preview = (chunk.text if hasattr(chunk, 'text') else chunk.get('text', ''))[:30]

        # ê°ì • ì´ëª¨ì§€ ë§¤í•‘
        emotion_emoji = {
            "ê¸°ì¨": "ğŸ˜Š",
            "ìŠ¬í””": "ğŸ˜¢",
            "ë¶„ë…¸": "ğŸ˜ ",
            "ë‘ë ¤ì›€": "ğŸ˜¨",
            "ë†€ëŒ": "ğŸ˜²",
            "í˜ì˜¤": "ğŸ˜–",
            "neutral": "ğŸ˜"
        }

        emoji = emotion_emoji.get(emotion, "â“")

        line = f"{i+1:3d}. {emoji} {emotion:10s} | {text_preview}..."

        # ì „í™˜ì  í‘œì‹œ
        if hasattr(chunk, 'metadata') and chunk.metadata:
            if chunk.metadata.get('is_transition_point'):
                sig = chunk.metadata.get('transition_significance', 0)
                line += f" [ì „í™˜ì  â˜…x{sig}]"

        lines.append(line)

    lines.append("=" * 50)
    return "\n".join(lines)


def benchmark_chunking_strategies(
    text: str,
    strategies: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ ì²­í‚¹ ì „ëµì„ ë²¤ì¹˜ë§ˆí¬í•©ë‹ˆë‹¤.

    Args:
        text: í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
        strategies: ì „ëµ ì„¤ì • ë¦¬ìŠ¤íŠ¸
                   [{"name": "strategy1", "params": {...}}, ...]

    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    import time
    from .emotion_chunker import split_text_by_emotions

    results = {}

    for strategy in strategies:
        name = strategy.get("name", "unknown")
        params = strategy.get("params", {})

        start_time = time.time()

        try:
            chunks = split_text_by_emotions(text, **params)
            elapsed = time.time() - start_time

            stats = get_emotion_statistics(chunks)
            stats["execution_time"] = elapsed
            stats["strategy_name"] = name

            results[name] = stats

        except Exception as e:
            results[name] = {"error": str(e)}

    return results
