import asyncio
from typing import Dict, Any, List, TypedDict
from langgraph.graph import StateGraph, END
from services.split_text import split_text_with_sliding_window
from services.analyze_emotions_with_gpt import EmotionAnalysisResult, analyze_emotions_with_gpt
from services.async_music_generation import process_all_chunks_async
from services.mysql_service import mysql_service
from services import prompt_service
from utils.logger import log
from config import (
    GEN_DURATION,
    CHUNKS_PER_PAGE,
    MAX_CONCURRENT_EMOTION_ANALYSIS,
    MIN_CHUNK_SIZE,
    MAX_CHUNK_SIZE
)


class WorkflowState(TypedDict):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    text: str
    user_name: str
    book_title: str
    book_id: str
    book_dir: str
    
    # ì¤‘ê°„ ì²˜ë¦¬ ê²°ê³¼
    physical_chunks: List[str]
    emotion_analyses: List[Dict[str, Any]]
    final_chunks: List[Dict[str, Any]]
    chunk_metadata: List[Dict[str, Any]]
    
    # ìµœì¢… ê²°ê³¼
    page_chunk_mapping: Dict[int, Dict[str, Any]]
    page_results: List[Dict[str, Any]]
    total_duration: int
    successful_pages: int
    
    # ë©”íƒ€ë°ì´í„°
    processing_times: Dict[str, float]
    errors: List[str]


class MusicGenerationWorkflow:
    """LangGraph ê¸°ë°˜ ìŒì•… ìƒì„± ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        workflow = StateGraph(WorkflowState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("split_text", self._split_text_node)
        workflow.add_node("analyze_emotions", self._analyze_emotions_node)
        workflow.add_node("create_final_chunks", self._create_final_chunks_node)
        workflow.add_node("generate_music", self._generate_music_node)
        workflow.add_node("save_to_database", self._save_to_database_node)
        workflow.add_node("create_page_mapping", self._create_page_mapping_node)
        
        # ì—£ì§€ ì—°ê²°
        workflow.set_entry_point("split_text")
        workflow.add_edge("split_text", "analyze_emotions")
        workflow.add_edge("analyze_emotions", "create_final_chunks")
        workflow.add_edge("create_final_chunks", "generate_music")
        workflow.add_edge("generate_music", "create_page_mapping")
        workflow.add_edge("create_page_mapping", "save_to_database")
        workflow.add_edge("save_to_database", END)
        
        return workflow.compile()
    
    async def _split_text_node(self, state: WorkflowState) -> WorkflowState:
        """1ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ ë¬¼ë¦¬ì  ì²­í¬ë¡œ ë¶„ë¦¬"""
        import time
        start_time = time.time()
        
        log("ğŸ“– LangGraph: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹œì‘")
        
        try:
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
            physical_chunks = split_text_with_sliding_window(
                state["text"], 
                max_size=5000,  # num_ctxì— ë§ì¶˜ ìµœëŒ€ ì²­í¬ í¬ê¸°
                overlap=300     # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•œ ì˜¤ë²„ë©
            )
            
            elapsed_time = time.time() - start_time
            log(f"âœ… LangGraph: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì™„ë£Œ - {len(physical_chunks)}ê°œ ì²­í¬ ({elapsed_time:.2f}ì´ˆ)")
            
            return {
                **state,
                "physical_chunks": physical_chunks,
                "processing_times": {**state.get("processing_times", {}), "split_text": elapsed_time}
            }
            
        except Exception as e:
            log(f"âŒ LangGraph: í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹¤íŒ¨ - {e}")
            return {
                **state,
                "errors": state.get("errors", []) + [f"í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹¤íŒ¨: {e}"]
            }
    
    async def _analyze_emotions_node(self, state: WorkflowState) -> WorkflowState:
        """2ë‹¨ê³„: ëª¨ë“  ì²­í¬ë¥¼ ë¹„ë™ê¸°ë¡œ ê°ì • ë¶„ì„"""
        import time
        start_time = time.time()
        
        log(f"ğŸ­ LangGraph: {len(state['physical_chunks'])}ê°œ ì²­í¬ ê°ì • ë¶„ì„ ì‹œì‘")
        
        try:
            # ëª¨ë“  ì²­í¬ë¥¼ ë¹„ë™ê¸°ë¡œ ê°ì • ë¶„ì„ (ë™ì‹œì„± ì œí•œ)
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_EMOTION_ANALYSIS)

            async def analyze_one(i: int, chunk_text: str) -> Dict[str, Any]:
                async with semaphore:
                    try:
                        # ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œ í’€ë¡œ ì‹¤í–‰í•˜ì—¬ ë³‘ë ¬í™”
                        result = await asyncio.to_thread(analyze_emotions_with_gpt, chunk_text)
                        return {
                            "chunk_index": i,
                            "text": chunk_text,
                            "analysis": result,
                            "success": True,
                        }
                    except Exception as e:
                        log(f"âŒ ì²­í¬ {i} ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                        return {
                            "chunk_index": i,
                            "text": chunk_text,
                            "error": str(e),
                            "success": False,
                        }

            tasks = [analyze_one(i, chunk) for i, chunk in enumerate(state["physical_chunks"])]
            emotion_analyses = await asyncio.gather(*tasks)
            
            elapsed_time = time.time() - start_time
            successful_count = len([a for a in emotion_analyses if a["success"]])
            log(f"âœ… LangGraph: ê°ì • ë¶„ì„ ì™„ë£Œ - ì„±ê³µ {successful_count}/{len(emotion_analyses)}ê°œ ({elapsed_time:.2f}ì´ˆ)")
            
            return {
                **state,
                "emotion_analyses": emotion_analyses,
                "processing_times": {**state.get("processing_times", {}), "analyze_emotions": elapsed_time}
            }
            
        except Exception as e:
            log(f"âŒ LangGraph: ê°ì • ë¶„ì„ ì‹¤íŒ¨ - {e}")
            return {
                **state,
                "errors": state.get("errors", []) + [f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}"]
            }
    
    async def _create_final_chunks_node(self, state: WorkflowState) -> WorkflowState:
        """3ë‹¨ê³„: ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ì²­í¬ ìƒì„±"""
        import time
        start_time = time.time()

        log("âœ‚ï¸ LangGraph: ìµœì¢… ì²­í¬ ìƒì„± ì‹œì‘")

        try:
            final_chunks = []
            skipped_chunks = 0
            merged_chunks = 0

            for analysis in state["emotion_analyses"]:
                if not analysis["success"]:
                    log(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨í•œ ì²­í¬ ê±´ë„ˆëœ€")
                    skipped_chunks += 1
                    continue

                emotional_phases = analysis["analysis"].get("emotional_phases", [])
                chunk_text = analysis["text"]

                # ê°ì • ì „í™˜ì ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
                if not emotional_phases:
                    if MIN_CHUNK_SIZE <= len(chunk_text) <= MAX_CHUNK_SIZE:
                        final_chunks.append({
                            "text": chunk_text,
                            "context": {"emotions": "neutral"}
                        })
                    else:
                        log(f"âš ï¸ ì²­í¬ í¬ê¸° ì´ˆê³¼ ({len(chunk_text)}ì), ê±´ë„ˆëœ€")
                        skipped_chunks += 1
                    continue

                # ê°ì • ì „í™˜ì ìœ¼ë¡œ ì„¸ë¶„í™”
                # position_in_full_textê°€ Noneì¸ í•­ëª© í•„í„°ë§
                valid_phases = [p for p in emotional_phases if p.get("position_in_full_text") is not None]

                if not valid_phases:
                    log(f"âš ï¸ ìœ íš¨í•œ positionì´ ì—†ìŒ, ì „ì²´ ì²­í¬ ì‚¬ìš©")
                    if MIN_CHUNK_SIZE <= len(chunk_text) <= MAX_CHUNK_SIZE:
                        final_chunks.append({
                            "text": chunk_text,
                            "context": {"emotions": "neutral"}
                        })
                    continue

                last_pos = 0
                temp_chunks = []

                for phase in valid_phases:
                    phase_pos = phase.get("position_in_full_text", 0)

                    if phase_pos <= last_pos:
                        log(f"âš ï¸ ì˜ëª»ëœ ìœ„ì¹˜ ìˆœì„œ: {phase_pos} <= {last_pos}, ê±´ë„ˆëœ€")
                        continue

                    sub_chunk_text = chunk_text[last_pos:phase_pos].strip()

                    # ìµœì†Œ í¬ê¸° ê²€ì¦
                    if len(sub_chunk_text) < MIN_CHUNK_SIZE:
                        log(f"âš ï¸ ë„ˆë¬´ ì‘ì€ ì²­í¬ ({len(sub_chunk_text)}ì), ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•© ì˜ˆì •")
                        # ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©ì„ ìœ„í•´ last_posë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                        continue

                    # ìµœëŒ€ í¬ê¸° ê²€ì¦
                    if len(sub_chunk_text) > MAX_CHUNK_SIZE:
                        log(f"âš ï¸ ë„ˆë¬´ í° ì²­í¬ ({len(sub_chunk_text)}ì), ë¶„í•  í•„ìš”")
                        # TODO: í° ì²­í¬ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
                        skipped_chunks += 1
                        last_pos = phase_pos
                        continue

                    temp_chunks.append({
                        "text": sub_chunk_text,
                        "context": {
                            "emotions": phase.get("emotions_before", "unknown"),
                            "transition": phase.get("emotions_after", "unknown"),
                            "significance": phase.get("significance", 1),
                            "explanation": phase.get("explanation", "")
                        }
                    })
                    last_pos = phase_pos

                # ë§ˆì§€ë§‰ ë‚¨ì€ ë¶€ë¶„ ì²˜ë¦¬
                if last_pos < len(chunk_text):
                    final_text = chunk_text[last_pos:].strip()

                    if len(final_text) >= MIN_CHUNK_SIZE:
                        if len(final_text) <= MAX_CHUNK_SIZE:
                            temp_chunks.append({
                                "text": final_text,
                                "context": {
                                    "emotions": valid_phases[-1].get("emotions_after", "unknown") if valid_phases else "neutral"
                                }
                            })
                        else:
                            log(f"âš ï¸ ë§ˆì§€ë§‰ ì²­í¬ í¬ê¸° ì´ˆê³¼ ({len(final_text)}ì)")
                            skipped_chunks += 1
                    elif temp_chunks:
                        # ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì™€ ë³‘í•©
                        log(f"âœ‚ï¸ ë§ˆì§€ë§‰ ì²­í¬ê°€ ì‘ì•„ì„œ ì´ì „ ì²­í¬ì™€ ë³‘í•© ({len(final_text)}ì)")
                        temp_chunks[-1]["text"] += " " + final_text
                        merged_chunks += 1

                final_chunks.extend(temp_chunks)

            # í†µê³„ ë¡œê¹…
            elapsed_time = time.time() - start_time
            log(f"âœ… LangGraph: ìµœì¢… ì²­í¬ ìƒì„± ì™„ë£Œ")
            log(f"   - ìƒì„±: {len(final_chunks)}ê°œ")
            log(f"   - ê±´ë„ˆëœ€: {skipped_chunks}ê°œ")
            log(f"   - ë³‘í•©: {merged_chunks}ê°œ")
            log(f"   - ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

            return {
                **state,
                "final_chunks": final_chunks,
                "processing_times": {**state.get("processing_times", {}), "create_final_chunks": elapsed_time}
            }

        except Exception as e:
            log(f"âŒ LangGraph: ìµœì¢… ì²­í¬ ìƒì„± ì‹¤íŒ¨ - {e}")
            import traceback
            log(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return {
                **state,
                "errors": state.get("errors", []) + [f"ìµœì¢… ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}"]
            }
    
    async def _generate_music_node(self, state: WorkflowState) -> WorkflowState:
        """4ë‹¨ê³„: ëª¨ë“  ì²­í¬ë¥¼ ë¹„ë™ê¸°ë¡œ ìŒì•… ìƒì„±"""
        import time
        start_time = time.time()
        
        log(f"ğŸµ LangGraph: {len(state['final_chunks'])}ê°œ ì²­í¬ ìŒì•… ìƒì„± ì‹œì‘")
        
        try:
            # ê¸€ë¡œë²Œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            global_prompt = prompt_service.generate_global(state["text"])
            
            # ëª¨ë“  ì²­í¬ë¥¼ ë¹„ë™ê¸°ë¡œ ìŒì•… ìƒì„±
            chunk_metadata = await process_all_chunks_async(
                state["final_chunks"], 
                state["book_dir"], 
                global_prompt
            )
            
            elapsed_time = time.time() - start_time
            log(f"âœ… LangGraph: ìŒì•… ìƒì„± ì™„ë£Œ - {len(chunk_metadata)}ê°œ ì„±ê³µ ({elapsed_time:.2f}ì´ˆ)")
            
            return {
                **state,
                "chunk_metadata": chunk_metadata,
                "processing_times": {**state.get("processing_times", {}), "generate_music": elapsed_time}
            }
            
        except Exception as e:
            log(f"âŒ LangGraph: ìŒì•… ìƒì„± ì‹¤íŒ¨ - {e}")
            return {
                **state,
                "errors": state.get("errors", []) + [f"ìŒì•… ìƒì„± ì‹¤íŒ¨: {e}"]
            }
    
    async def _create_page_mapping_node(self, state: WorkflowState) -> WorkflowState:
        """5ë‹¨ê³„: í˜ì´ì§€ë³„ ì²­í¬ ë§¤í•‘ ìƒì„±"""
        import time
        start_time = time.time()
        
        log("ğŸ“„ LangGraph: í˜ì´ì§€ ë§¤í•‘ ìƒì„± ì‹œì‘")
        
        try:
            # í˜ì´ì§€ë³„ ì²­í¬ ë§¤í•‘ ìƒì„± (í•œ í˜ì´ì§€ë‹¹ ê³ ì • ì²­í¬ ìˆ˜)
            page_chunk_mapping: Dict[int, Dict[str, Any]] = {}
            
            for i, chunk in enumerate(state["chunk_metadata"]):
                page_num = (i // CHUNKS_PER_PAGE) + 1
                if page_num not in page_chunk_mapping:
                    page_chunk_mapping[page_num] = {
                        "start_index": i + 1,
                        "end_index": i + 1,
                        "chunk_count": 0
                    }
                page_chunk_mapping[page_num]["end_index"] = i + 1
                page_chunk_mapping[page_num]["chunk_count"] += 1
                chunk["page"] = page_num
            
            elapsed_time = time.time() - start_time
            log(f"âœ… LangGraph: í˜ì´ì§€ ë§¤í•‘ ìƒì„± ì™„ë£Œ - {len(page_chunk_mapping)}í˜ì´ì§€ ({elapsed_time:.2f}ì´ˆ)")
            
            return {
                **state,
                "page_chunk_mapping": page_chunk_mapping,
                "processing_times": {**state.get("processing_times", {}), "create_page_mapping": elapsed_time}
            }
            
        except Exception as e:
            log(f"âŒ LangGraph: í˜ì´ì§€ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨ - {e}")
            return {
                **state,
                "errors": state.get("errors", []) + [f"í˜ì´ì§€ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨: {e}"]
            }
    
    async def _save_to_database_node(self, state: WorkflowState) -> WorkflowState:
        """6ë‹¨ê³„: DBì— ì €ì¥"""
        import time
        start_time = time.time()
        
        log("ğŸ’¾ LangGraph: DB ì €ì¥ ì‹œì‘")
        
        try:
            page_results: List[Dict[str, Any]] = []
            
            for page_num, mapping in state["page_chunk_mapping"].items():
                start_idx = mapping["start_index"] - 1
                end_idx = mapping["end_index"]
                
                page_chunks = state["chunk_metadata"][start_idx:end_idx]
                
                if not page_chunks:
                    page_results.append({
                        "page": page_num,
                        "chunks": 0,
                        "duration": 0,
                        "error": "ì²­í¬ ìƒì„± ì‹¤íŒ¨"
                    })
                    continue
                
                page_duration = len(page_chunks) * GEN_DURATION
                
                try:
                    mysql_service.save_chapter_chunks(
                        book_id=state["book_id"],
                        page=page_num,
                        chunks=page_chunks,
                        total_duration=page_duration,
                        book_title=state["book_title"],
                    )
                    
                    page_results.append({
                        "page": page_num,
                        "chunks": len(page_chunks),
                        "duration": page_duration,
                        "cached": False
                    })
                    
                except Exception as e:
                    page_results.append({
                        "page": page_num,
                        "error": str(e),
                        "cached": False
                    })
            
            total_duration = sum(page.get("duration", 0) for page in page_results)
            successful_pages = len([p for p in page_results if "error" not in p])
            
            elapsed_time = time.time() - start_time
            log(f"âœ… LangGraph: DB ì €ì¥ ì™„ë£Œ - {successful_pages}í˜ì´ì§€ ì„±ê³µ ({elapsed_time:.2f}ì´ˆ)")
            
            return {
                **state,
                "page_results": page_results,
                "total_duration": total_duration,
                "successful_pages": successful_pages,
                "processing_times": {**state.get("processing_times", {}), "save_to_database": elapsed_time}
            }
            
        except Exception as e:
            log(f"âŒ LangGraph: DB ì €ì¥ ì‹¤íŒ¨ - {e}")
            return {
                **state,
                "errors": state.get("errors", []) + [f"DB ì €ì¥ ì‹¤íŒ¨: {e}"]
            }
    
    async def run_workflow(self, text: str, user_name: str, book_title: str, book_id: str, book_dir: str) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        log("ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹œì‘")
        
        initial_state = WorkflowState(
            text=text,
            user_name=user_name,
            book_title=book_title,
            book_id=book_id,
            book_dir=book_dir,
            physical_chunks=[],
            emotion_analyses=[],
            final_chunks=[],
            chunk_metadata=[],
            page_chunk_mapping={},
            page_results=[],
            total_duration=0,
            successful_pages=0,
            processing_times={},
            errors=[]
        )
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        final_state = await self.graph.ainvoke(initial_state)
        
        log("ğŸ‰ LangGraph ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
        
        return {
            "message": f"{book_title} ìŒì•… ìƒì„± ì™„ë£Œ (LangGraph)",
            "book_id": book_id,
            "text_length": len(text),
            "total_pages": len(final_state["page_chunk_mapping"]),
            "total_chunks": len(final_state["chunk_metadata"]),
            "total_duration": final_state["total_duration"],
            "successful_pages": final_state["successful_pages"],
            "pages": final_state["page_results"],
            "processing_method": "langgraph_workflow",
            "processing_times": final_state["processing_times"],
            "errors": final_state["errors"]
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
music_workflow = MusicGenerationWorkflow()
